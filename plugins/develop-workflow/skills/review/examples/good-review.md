# 좋은 리뷰 예시

이 문서는 고품질 리뷰의 예시를 보여줍니다.

---

## 예시 1: 기술 계획 리뷰 (엔지니어 관점)

### 요청
> "staff+ 엔지니어 관점으로 plans 디렉토리를 리뷰해줘. 우리는 3명 스타트업 팀이고 빠른 MVP가 필요해"

### 리뷰 결과

**관점**: Staff+ 소프트웨어 엔지니어 (10년+ 경력)
**대상 파일**: plans/*.md
**컨텍스트**: 3명 스타트업 팀, 빠른 MVP 필요

---

## 요약

전반적으로 매우 실용적이고 체계적인 계획입니다. Phase 구분이 명확하고 우선순위 설정이 합리적입니다. 다만 스타트업 환경을 고려할 때 일부 과도한 엔지니어링이 우려되며, MVP에 집중하기 위한 범위 축소를 권장합니다.

---

## 강점

### 1. **명확한 Phase 구분과 우선순위**
- Phase 0-4로 명확히 분리되어 있어 순차적 실행이 용이
- 각 Phase의 목표와 산출물이 구체적으로 정의됨
- **영향**: 팀이 무엇을 언제 해야 할지 명확히 알 수 있음

### 2. **자연어 인터페이스 채택**
- 플래그 기반 대신 자연어를 사용
- LLM의 지능을 완전히 활용하여 관점, 파일, 컨텍스트를 자동 파악
- **영향**: 학습 곡선이 낮고 사용이 직관적

### 3. **다중 LLM 활용 전략**
- Claude, OpenAI Codex, Gemini를 모두 활용하여 다양한 관점 확보
- 단일 LLM 편향을 피하고 더 포괄적인 피드백 확보

---

## 약점 및 개선 영역

### 1. **MVP 범위 과다**
- 3명 팀에서 전체를 구현하려면 상당한 시간 소요
- **영향**: MVP 출시 지연 위험

### 2. **에러 핸들링 부족**
- API 호출 실패, 타임아웃 대응 없음
- **영향**: 사용자 경험 저하

---

## 권장사항

### Critical
1. **MVP 범위 축소**: Claude만 먼저, 검증 후 Codex/Gemini 추가
2. **에러 핸들링 강화**: API 실패 처리, 사용자 피드백

### Important
3. **점진적 구현**: 기본 기능부터 단계적 확장
4. **테스트 계획 구체화**: 자연어 해석 품질 검증

---

## 종합 평가

**점수**: 82/100

| 기준 | 점수 |
|------|------|
| 완성도 | 19/20 |
| 일관성 | 18/20 |
| 명확성 | 17/20 |
| 실현 가능성 | 13/20 |
| 품질 | 15/20 |

**핵심**: 기술적 탁월함을 인정하되, 현실(리소스, 시간)을 고려한 실용적 조언 제공.
