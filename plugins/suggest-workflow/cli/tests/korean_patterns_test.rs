use suggest_workflow::analyzers::suffix_miner::SuffixMiner;
use suggest_workflow::analyzers::tacit::analyze_tacit_knowledge;
use suggest_workflow::analyzers::depth::{AnalysisDepth, DepthConfig};
use suggest_workflow::analyzers::stopwords::StopwordSet;
use suggest_workflow::types::{HistoryEntry, TacitPattern};

/// Helper to construct HistoryEntry with display and timestamp
fn make_entry(display: &str, ts: i64) -> HistoryEntry {
    HistoryEntry {
        display: display.to_string(),
        timestamp: ts,
        project: "test-project".to_string(),
    }
}

/// Helper: resolve a DepthConfig with a custom similarity threshold
fn config_with_similarity(similarity: f64) -> DepthConfig {
    let mut config = AnalysisDepth::Normal.resolve();
    config.similarity_threshold = similarity;
    config
}

/// Default config for most tests
fn default_config() -> DepthConfig {
    config_with_similarity(0.3)
}

// ============================================================================
// SUFFIX MINER UNIT TESTS
// ============================================================================

#[test]
fn test_suffix_miner_basic_korean() {
    let prompts = vec![
        "íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜",
        "íƒ€ì…ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”",
        "ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•´ì¤˜",
        "ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•´ì£¼ì„¸ìš”",
    ];

    let miner = SuffixMiner::default();
    let suffixes = miner.mine(&prompts);

    // Should find common Korean suffixes
    assert!(!suffixes.is_empty(), "Should discover Korean suffixes");

    // Check that suffixes contain expected patterns
    let suffix_strs: Vec<String> = suffixes.iter().map(|s| s.text.clone()).collect();
    let has_haejwo = suffix_strs.iter().any(|s| s.contains("í•´ì¤„") || s.contains("í•´ì£¼ì„¸ìš”"));
    assert!(has_haejwo, "Should find í•´ì¤˜/í•´ì£¼ì„¸ìš” suffix patterns");
}

#[test]
fn test_suffix_miner_frequency_threshold() {
    let prompts = vec![
        "íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜",
        "ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•´ì¤˜",
        "ì£¼ì„ì„ ì¶”ê°€í•´ì¤˜",
        "ì½”ë“œë¥¼ ë¦¬íŒ©í† ë§í•˜ì„¸ìš”",
    ];

    let miner_low = SuffixMiner::new(2, 10, 0.1);  // Lower threshold
    let miner_high = SuffixMiner::new(2, 10, 0.5); // Higher threshold

    let suffixes_low = miner_low.mine(&prompts);
    let suffixes_high = miner_high.mine(&prompts);

    // Lower threshold should find more or equal suffixes (excluding fallbacks)
    let low_real: Vec<_> = suffixes_low.iter().filter(|s| s.frequency > 0).collect();
    let high_real: Vec<_> = suffixes_high.iter().filter(|s| s.frequency > 0).collect();

    assert!(
        low_real.len() >= high_real.len(),
        "Lower threshold should find more or equal real suffixes"
    );
}

#[test]
fn test_suffix_miner_empty_input() {
    let miner = SuffixMiner::default();
    let suffixes = miner.mine(&[]);
    assert!(suffixes.is_empty(), "Empty input should produce no suffixes");
}

#[test]
fn test_suffix_miner_single_prompt() {
    let prompts = vec!["íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜"];
    let miner = SuffixMiner::new(2, 10, 0.5);

    let suffixes = miner.mine(&prompts);
    // With small corpus, fallback suffixes are added
    // Filter to only real mined suffixes (frequency > 0)
    let real_suffixes: Vec<_> = suffixes.iter().filter(|s| s.frequency > 0).collect();
    assert!(real_suffixes.is_empty(), "Single prompt should not meet min_support threshold");
}

#[test]
fn test_suffix_miner_byte_safety() {
    // Various Korean strings with different byte lengths
    let prompts = vec![
        "ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬",
        "ğŸ˜€ ì´ëª¨ì§€ì™€ í•œê¸€",
        "æ··åˆ í•œê¸€ ä¸­æ–‡ text",
        "ã„±ã„´ã„·ã„¹ã…ã…‚ã……",
        "",
    ];

    let miner = SuffixMiner::default();
    // Should not panic
    let _suffixes = miner.mine(&prompts);
}

// ============================================================================
// NORMALIZATION TESTS
// ============================================================================

#[test]
fn test_normalization_strips_korean_suffixes() {
    let variations = vec![
        "íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜",
        "íƒ€ì…ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”",
        "íƒ€ì…ì„ ëª…ì‹œí•˜ì„¸ìš”",
    ];

    let miner = SuffixMiner::default();
    let suffixes = miner.mine(&variations);

    // Normalize each variation
    let normalized: Vec<_> = variations.iter()
        .map(|v| miner.normalize(v, &suffixes))
        .collect();

    // All variations should normalize to similar content
    assert!(!normalized.is_empty(), "Should produce normalized prompts");

    // Check that normalized versions strip polite endings
    for norm in &normalized {
        assert!(
            !norm.content.contains("í•´ì¤˜") &&
            !norm.content.contains("í•´ì£¼ì„¸ìš”") &&
            !norm.content.contains("í•˜ì„¸ìš”"),
            "Normalized content should strip polite suffixes"
        );
        assert!(norm.content.contains("íƒ€ì…") && norm.content.contains("ëª…ì‹œ"),
                "Should preserve core content");
    }
}

#[test]
fn test_normalization_preserves_core_content() {
    let prompt = "í•­ìƒ íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜";
    let miner = SuffixMiner::default();
    let suffixes = miner.mine(&vec![prompt]);

    let normalized = miner.normalize(prompt, &suffixes);

    assert!(normalized.content.contains("íƒ€ì…") || normalized.content.contains("ëª…ì‹œ"),
            "Should preserve core content words");
}

#[test]
fn test_normalization_empty_after_stripping() {
    let only_suffixes = vec!["í•´ì¤˜", "í•´ì£¼ì„¸ìš”", "í•˜ì„¸ìš”"];
    let miner = SuffixMiner::default();
    let suffixes = miner.mine(&only_suffixes);

    for text in &only_suffixes {
        let normalized = miner.normalize(text, &suffixes);
        assert!(!normalized.original.is_empty(), "Should handle suffix-only prompts gracefully");
    }
}

// ============================================================================
// CLUSTERING TESTS
// ============================================================================

#[test]
fn test_clustering_similar_prompts_group_together() {
    let entries = vec![
        make_entry("í•­ìƒ íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 1000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”", 2000),
        make_entry("íƒ€ì… ëª…ì‹œí•´ì¤˜", 3000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•˜ì„¸ìš”", 4000),
        make_entry("íƒ€ì… ëª…ì‹œí•˜ì„¸ìš”", 5000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());

    assert!(!result.patterns.is_empty(), "Should find patterns");

    let has_cluster = result.patterns.iter().any(|p| p.examples.len() >= 2);
    assert!(has_cluster, "Should cluster similar prompts together");
}

#[test]
fn test_clustering_different_topics_separate() {
    let entries = vec![
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 1000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”", 2000),
        make_entry("íƒ€ì… ëª…ì‹œí•´ì¤˜", 3000),
        make_entry("ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•´ì¤˜", 4000),
        make_entry("ì—ëŸ¬ ì²˜ë¦¬í•´ì£¼ì„¸ìš”", 5000),
        make_entry("ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ì„¸ìš”", 6000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());

    let pattern_texts: Vec<String> = result.patterns.iter()
        .map(|p| p.pattern.clone())
        .collect();

    let has_type_pattern = pattern_texts.iter().any(|p| p.contains("íƒ€ì…"));
    let has_error_pattern = pattern_texts.iter().any(|p| p.contains("ì—ëŸ¬"));

    assert!(has_type_pattern || has_error_pattern,
            "Should identify distinct topic patterns");
}

// ============================================================================
// TYPE CLASSIFICATION TESTS
// ============================================================================

#[test]
fn test_type_classification_directive() {
    let entries = vec![
        make_entry("í•­ìƒ íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 1000),
        make_entry("ë°˜ë“œì‹œ íƒ€ì…ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”", 2000),
        make_entry("ê¼­ íƒ€ì…ì„ ëª…ì‹œí•˜ì„¸ìš”", 3000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 4000),
        make_entry("íƒ€ì… ëª…ì‹œí•´ì£¼ì„¸ìš”", 5000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());

    let has_directive = result.patterns.iter().any(|p| {
        p.pattern_type == "directive" ||
        p.pattern.contains("í•­ìƒ") ||
        p.pattern.contains("ë°˜ë“œì‹œ") ||
        p.pattern.contains("ê¼­")
    });

    assert!(has_directive, "Should identify directive patterns");
}

#[test]
fn test_type_classification_convention() {
    let entries = vec![
        make_entry("camelCaseë¡œ ì‘ì„±í•´ì¤˜", 1000),
        make_entry("camelCase ì‚¬ìš©í•´ì£¼ì„¸ìš”", 2000),
        make_entry("camelCaseë¡œ ì¨ì¤˜", 3000),
        make_entry("snake_case ì‚¬ìš©í•´ì¤˜", 4000),
        make_entry("snake_caseë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”", 5000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());

    let has_convention = result.patterns.iter().any(|p| {
        p.pattern_type == "convention" ||
        p.pattern.contains("camelCase") ||
        p.pattern.contains("snake_case")
    });

    assert!(has_convention, "Should identify convention patterns");
}

#[test]
fn test_type_classification_preference() {
    let entries = vec![
        make_entry("async/await ì„ í˜¸í•´ìš”", 1000),
        make_entry("async/await ì“°ëŠ” ê²Œ ì¢‹ì•„ìš”", 2000),
        make_entry("Promiseë³´ë‹¤ async/await", 3000),
        make_entry("async/await ì‚¬ìš© ì„ í˜¸", 4000),
        make_entry("async/awaitê°€ ì¢‹ì•„ìš”", 5000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());

    let has_preference = result.patterns.iter().any(|p| {
        p.pattern_type == "preference" ||
        p.pattern.contains("ì„ í˜¸") ||
        p.pattern.contains("ì¢‹ì•„")
    });

    assert!(has_preference, "Should identify preference patterns");
}

#[test]
fn test_type_classification_correction() {
    let entries = vec![
        make_entry("ì•„ë‹ˆì•¼, íƒ€ì…ì„ ëª…ì‹œí•´ì•¼ í•´", 1000),
        make_entry("ì•„ë‹ˆì•¼ ì—ëŸ¬ ì²˜ë¦¬ í•„ìš”í•´", 2000),
        make_entry("ì•„ë‹ˆì•¼, ë‹¤ì‹œ ì‘ì„±í•´ì¤˜", 3000),
        make_entry("ì˜ëª»ëì–´ ê·¸ê²Œ ì•„ë‹ˆë¼", 4000),
        make_entry("ì˜ëª»ëì–´ ìˆ˜ì •í•´ì¤˜", 5000),
        make_entry("ì˜ëª»ëì–´ ë‹¤ì‹œ í•´ì¤˜", 6000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());

    if result.patterns.is_empty() {
        assert!(result.total == entries.len(), "Should process all entries");
    } else {
        for p in &result.patterns {
            assert!(!p.pattern.is_empty(), "Pattern should not be empty");
        }
    }
}

// ============================================================================
// FULL PIPELINE INTEGRATION TESTS
// ============================================================================

#[test]
fn test_full_pipeline_korean_prompts() {
    let entries = vec![
        make_entry("í•­ìƒ íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 1000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”", 2000),
        make_entry("íƒ€ì… ëª…ì‹œí•´ì¤˜", 3000),
        make_entry("ë°˜ë“œì‹œ ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•´ì¤˜", 4000),
        make_entry("ì—ëŸ¬ ì²˜ë¦¬í•´ì¤˜", 5000),
        make_entry("ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•´ì£¼ì„¸ìš”", 6000),
        make_entry("ì£¼ì„ì„ ì¶”ê°€í•´ì¤˜", 7000),
        make_entry("ì£¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”", 8000),
        make_entry("ì£¼ì„ì„ ë‹¬ì•„ì¤˜", 9000),
        make_entry("ì½”ë“œ ë¦¬ë·°í•´ì¤˜", 10000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());

    assert!(result.patterns.len() > 0, "Should produce patterns");
    assert_eq!(result.total, entries.len(), "Should count all entries");

    for p in &result.patterns {
        assert!(
            p.confidence >= 0.0 && p.confidence <= 1.0,
            "Confidence should be between 0 and 1, got {}", p.confidence
        );
        assert!(p.count >= 2, "Pattern count should meet minimum threshold");
        assert!(!p.pattern.is_empty(), "Pattern should not be empty");
        assert!(!p.examples.is_empty(), "Should have examples");
    }
}

#[test]
fn test_full_pipeline_with_bm25_ranking() {
    let entries = vec![
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 1000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”", 2000),
        make_entry("íƒ€ì… ëª…ì‹œí•´ì¤˜", 3000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•˜ì„¸ìš”", 4000),
        make_entry("íƒ€ì… ëª…ì‹œí•´ì£¼ì„¸ìš”", 5000),
        make_entry("ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•´ì¤˜", 6000),
        make_entry("ì—ëŸ¬ ì²˜ë¦¬í•´ì£¼ì„¸ìš”", 7000),
        make_entry("ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ì„¸ìš”", 8000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());

    assert!(!result.patterns.is_empty(), "Should find patterns");

    for p in &result.patterns {
        assert!(p.bm25_score >= 0.0, "BM25 score should be non-negative");
    }

    if result.patterns.len() >= 2 {
        let max_score = result.patterns.iter().map(|p| p.bm25_score).fold(0.0, f64::max);
        let min_score = result.patterns.iter().map(|p| p.bm25_score).fold(f64::MAX, f64::min);
        assert!(max_score >= min_score, "Should have valid BM25 score range");
    }
}

#[test]
fn test_full_pipeline_confidence_calculation() {
    let entries = vec![
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 1000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”", 2000),
        make_entry("íƒ€ì… ëª…ì‹œí•´ì¤˜", 3000),
        make_entry("ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•´ì¤˜", 4000),
        make_entry("ì—ëŸ¬ ì²˜ë¦¬í•´ì£¼ì„¸ìš”", 5000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());

    if result.patterns.len() >= 2 {
        let higher_count_pattern = result.patterns.iter()
            .max_by_key(|p| p.count)
            .unwrap();
        let lower_count_pattern = result.patterns.iter()
            .min_by_key(|p| p.count)
            .unwrap();

        if higher_count_pattern.count > lower_count_pattern.count {
            assert!(
                higher_count_pattern.confidence >= lower_count_pattern.confidence,
                "Higher count should generally yield higher confidence"
            );
        }
    }
}

#[test]
fn test_full_pipeline_respects_min_threshold() {
    let entries = vec![
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 1000),
        make_entry("ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•´ì¤˜", 2000),
        make_entry("ì£¼ì„ì„ ì¶”ê°€í•´ì¤˜", 3000),
        make_entry("ì½”ë“œ ë¦¬ë·°í•´ì¤˜", 4000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());

    for p in &result.patterns {
        assert!(
            p.count >= 2,
            "All patterns should meet minimum threshold of 2"
        );
    }
}

#[test]
fn test_full_pipeline_respects_max_patterns() {
    let entries = vec![
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 1000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”", 2000),
        make_entry("ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•´ì¤˜", 3000),
        make_entry("ì—ëŸ¬ ì²˜ë¦¬í•´ì£¼ì„¸ìš”", 4000),
        make_entry("ì£¼ì„ì„ ì¶”ê°€í•´ì¤˜", 5000),
        make_entry("ì£¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”", 6000),
        make_entry("ì½”ë“œ ë¦¬ë·°í•´ì¤˜", 7000),
        make_entry("ì½”ë“œë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”", 8000),
        make_entry("í…ŒìŠ¤íŠ¸ ì‘ì„±í•´ì¤˜", 9000),
        make_entry("í…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”", 10000),
    ];

    let max_patterns = 3;
    let result = analyze_tacit_knowledge(&entries, 2, max_patterns, &default_config(), false, 14.0, &StopwordSet::builtin());

    assert!(
        result.patterns.len() <= max_patterns,
        "Should respect max_patterns limit, got {} patterns", result.patterns.len()
    );
}

// ============================================================================
// DEPTH PRESET TESTS
// ============================================================================

#[test]
fn test_depth_narrow_produces_results() {
    let entries = vec![
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 1000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”", 2000),
        make_entry("íƒ€ì… ëª…ì‹œí•´ì¤˜", 3000),
        make_entry("ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•´ì¤˜", 4000),
        make_entry("ì—ëŸ¬ ì²˜ë¦¬í•´ì£¼ì„¸ìš”", 5000),
    ];

    let config = AnalysisDepth::Narrow.resolve();
    let result = analyze_tacit_knowledge(&entries, 2, 10, &config, false, 14.0, &StopwordSet::builtin());

    assert_eq!(result.total, entries.len());
    // Narrow has higher similarity threshold so may find fewer patterns
}

#[test]
fn test_depth_wide_finds_more_patterns() {
    let entries = vec![
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 1000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”", 2000),
        make_entry("íƒ€ì… ëª…ì‹œí•´ì¤˜", 3000),
        make_entry("ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•´ì¤˜", 4000),
        make_entry("ì—ëŸ¬ ì²˜ë¦¬í•´ì£¼ì„¸ìš”", 5000),
        make_entry("ì£¼ì„ì„ ì¶”ê°€í•´ì¤˜", 6000),
        make_entry("ì£¼ì„ ì¶”ê°€í•´ì¤˜", 7000),
    ];

    let narrow = analyze_tacit_knowledge(&entries, 2, 10, &AnalysisDepth::Narrow.resolve(), false, 14.0, &StopwordSet::builtin());
    let wide = analyze_tacit_knowledge(&entries, 2, 10, &AnalysisDepth::Wide.resolve(), false, 14.0, &StopwordSet::builtin());

    // Wide (lower similarity threshold) should merge more clusters â†’ potentially fewer but larger patterns
    // Or with lower thresholds, discover more. Just verify both work.
    assert_eq!(narrow.total, wide.total);
}

#[test]
fn test_depth_config_values() {
    let narrow = AnalysisDepth::Narrow.resolve();
    let normal = AnalysisDepth::Normal.resolve();
    let wide = AnalysisDepth::Wide.resolve();

    // Narrow should be more conservative
    assert!(narrow.sentence_split_min_tokens > normal.sentence_split_min_tokens);
    assert!(normal.sentence_split_min_tokens > wide.sentence_split_min_tokens);

    assert!(narrow.idf_top_k < normal.idf_top_k);
    assert!(normal.idf_top_k < wide.idf_top_k);

    assert!(narrow.max_sub_queries < normal.max_sub_queries);
    assert!(normal.max_sub_queries < wide.max_sub_queries);

    assert!(narrow.similarity_threshold > normal.similarity_threshold);
    assert!(normal.similarity_threshold > wide.similarity_threshold);
}

#[test]
fn test_depth_from_str() {
    assert_eq!("narrow".parse::<AnalysisDepth>().unwrap(), AnalysisDepth::Narrow);
    assert_eq!("normal".parse::<AnalysisDepth>().unwrap(), AnalysisDepth::Normal);
    assert_eq!("wide".parse::<AnalysisDepth>().unwrap(), AnalysisDepth::Wide);
    assert_eq!("WIDE".parse::<AnalysisDepth>().unwrap(), AnalysisDepth::Wide);
    assert!("invalid".parse::<AnalysisDepth>().is_err());
}

// ============================================================================
// BYTE SAFETY TESTS
// ============================================================================

#[test]
fn test_byte_safety_mixed_unicode() {
    let entries = vec![
        make_entry("ğŸ˜€ íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 1000),
        make_entry("ğŸ‰ ì—ëŸ¬ ì²˜ë¦¬í•´ì£¼ì„¸ìš”", 2000),
        make_entry("æ··åˆ í•œê¸€ ä¸­æ–‡ text", 3000),
        make_entry("ã„±ã„´ã„· ììŒë§Œ", 4000),
        make_entry("ğŸ”¥ğŸ”¥ğŸ”¥", 5000),
    ];

    let _result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());
}

#[test]
fn test_byte_safety_zero_width_chars() {
    let entries = vec![
        make_entry("íƒ€ì…\u{200B}ì„ ëª…ì‹œí•´ì¤˜", 1000),
        make_entry("íƒ€\u{FEFF}ì…ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”", 2000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 3000),
    ];

    let _result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());
}

#[test]
fn test_byte_safety_long_korean_text() {
    let long_text = "íƒ€ì…ì„ ".repeat(100) + "ëª…ì‹œí•´ì¤˜";
    let entries = vec![
        make_entry(&long_text, 1000),
        make_entry(&long_text, 2000),
        make_entry("ì—ëŸ¬ ì²˜ë¦¬í•´ì¤˜", 3000),
    ];

    let _result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());
}

// ============================================================================
// EDGE CASES
// ============================================================================

#[test]
fn test_edge_case_empty_input() {
    let entries: Vec<HistoryEntry> = vec![];
    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());

    assert_eq!(result.total, 0, "Should handle empty input");
    assert!(result.patterns.is_empty(), "Should produce no patterns for empty input");
}

#[test]
fn test_edge_case_single_prompt() {
    let entries = vec![
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 1000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());

    assert_eq!(result.total, 1, "Should count single entry");
    assert!(
        result.patterns.is_empty(),
        "Single prompt should not meet threshold of 2"
    );
}

#[test]
fn test_edge_case_all_confirmation_prompts() {
    let entries = vec![
        make_entry("ë„¤", 1000),
        make_entry("ì‘", 2000),
        make_entry("ì•Œê² ì–´", 3000),
        make_entry("ì¢‹ì•„", 4000),
        make_entry("í™•ì¸", 5000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());

    assert_eq!(result.total, entries.len());
    for p in &result.patterns {
        assert!(!p.pattern.is_empty(), "Patterns should not be empty");
    }
}

#[test]
fn test_edge_case_identical_prompts() {
    let entries = vec![
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 1000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 2000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 3000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 4000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 5000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());

    assert!(!result.patterns.is_empty(), "Should find pattern from identical prompts");

    let strong_patterns: Vec<&TacitPattern> = result.patterns.iter()
        .filter(|p| p.count >= 5)
        .collect();

    assert!(!strong_patterns.is_empty(), "Should have at least one strong pattern");
}

#[test]
fn test_edge_case_very_short_prompts() {
    let entries = vec![
        make_entry("íƒ€ì…", 1000),
        make_entry("ì—ëŸ¬", 2000),
        make_entry("í…ŒìŠ¤íŠ¸", 3000),
        make_entry("ì£¼ì„", 4000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());
    assert_eq!(result.total, entries.len());
}

#[test]
fn test_edge_case_prompts_with_only_whitespace() {
    let entries = vec![
        make_entry("   ", 1000),
        make_entry("\t\t", 2000),
        make_entry("\n\n", 3000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 4000),
        make_entry("ì—ëŸ¬ ì²˜ë¦¬í•´ì¤˜", 5000),
    ];

    let result = analyze_tacit_knowledge(&entries, 2, 10, &default_config(), false, 14.0, &StopwordSet::builtin());
    assert_eq!(result.total, entries.len());
}

#[test]
fn test_edge_case_similarity_threshold_variation() {
    let entries = vec![
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì¤˜", 1000),
        make_entry("íƒ€ì…ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”", 2000),
        make_entry("ì—ëŸ¬ ì²˜ë¦¬í•´ì¤˜", 3000),
    ];

    let result_high = analyze_tacit_knowledge(&entries, 2, 10, &config_with_similarity(0.9), false, 14.0, &StopwordSet::builtin());
    let result_low = analyze_tacit_knowledge(&entries, 2, 10, &config_with_similarity(0.1), false, 14.0, &StopwordSet::builtin());

    // Higher similarity threshold = stricter clustering = more groups
    // Lower similarity threshold = more aggressive merging = fewer groups
    // Both should work without error
    assert!(result_high.total == result_low.total);
}
